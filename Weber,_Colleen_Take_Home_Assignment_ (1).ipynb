{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "# Used Chat GPT and Google Colab AI\n",
        "# Used the in-class code as well"
      ],
      "metadata": {
        "id": "BzipI2t_o96d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HazHSOkVpKt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8m.pt\")"
      ],
      "metadata": {
        "id": "hbeH8dszpQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Load image path and allow for multiple extensions\n",
        "image_path = '/content/'\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# Get all image files in the directory - already added into Colab\n",
        "images = []\n",
        "for ext in image_extensions:\n",
        "    images.extend(glob(os.path.join(image_path, ext)))\n"
      ],
      "metadata": {
        "id": "hE2qwn5rs4c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for img in images:\n",
        "  results = model.predict(img)\n",
        "\n",
        "  result = results[0]\n",
        "  i += 1\n",
        "\n",
        "# Display the image with object detection\n",
        "  annotated_image = result.plot()\n",
        "  plt.imshow(annotated_image)\n",
        "  plt.axis('off')  # for better readability\n",
        "  plt.show() # display\n",
        "  annotated_image_pil = Image.fromarray(annotated_image)\n",
        "  annotated_image_pil.save(f\"{i}.jpg\")\n",
        "  for box in result.boxes:\n",
        "    box = result.boxes[0]\n",
        "    print(\"Object type:\", box.cls)\n",
        "    print(\"Coordinates:\", box.xyxy)\n",
        "    print(\"Probability:\", box.conf)\n",
        "    class_id = result.names[box.cls[0].item()]\n",
        "    cords = box.xyxy[0].tolist()\n",
        "    cords = [round(x) for x in cords]\n",
        "    conf = round(box.conf[0].item(), 2)\n",
        "    print(\"Object type:\", class_id) #the ID of object type\n",
        "    print(\"Coordinates:\", cords) #the coordinates of the box as an array [x1,y1,x2,y2]\n",
        "    print(\"Probability:\", conf) # confidence\n",
        "    print(\"\\nNext Object\")\n"
      ],
      "metadata": {
        "id": "GS6WsKdAs94K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: extract the times and accuracies from the above analysis and make into an organized table\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "\n",
        "!pip install ultralytics\n",
        "\n",
        "# Used Chat GPT and Google Colab AI\n",
        "# Used the in-class code as well\n",
        "model = YOLO(\"yolov8m.pt\")\n",
        "\n",
        "# Load image path\n",
        "image_path = '/content/'\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "\n",
        "# Get all image files in the directory\n",
        "images = []\n",
        "for ext in image_extensions:\n",
        "    images.extend(glob(os.path.join(image_path, ext)))\n",
        "\n",
        "data = []\n",
        "i = 0\n",
        "for img in images:\n",
        "  results = model.predict(img)\n",
        "  result = results[0]\n",
        "  i += 1\n",
        "\n",
        "  # Display the image with object detection\n",
        "  annotated_image = result.plot()\n",
        "  plt.imshow(annotated_image)\n",
        "  plt.axis('off')  # for better readability\n",
        "  plt.show() # display\n",
        "  annotated_image_pil = Image.fromarray(annotated_image)\n",
        "  annotated_image_pil.save(f\"{i}.jpg\")\n",
        "  for box in result.boxes:\n",
        "    box = result.boxes[0]\n",
        "    class_id = result.names[box.cls[0].item()]\n",
        "    cords = box.xyxy[0].tolist()\n",
        "    cords = [round(x) for x in cords]\n",
        "    conf = round(box.conf[0].item(), 2)\n",
        "    data.append([i, class_id, cords, conf]) # Store data in a list of lists\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Image', 'Object Type', 'Coordinates', 'Probability'])\n",
        "df"
      ],
      "metadata": {
        "id": "ne4n83kyJWhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Performance metrics\n",
        "performance_data = []\n",
        "\n",
        "# Start counting at 0\n",
        "i = 0\n",
        "\n",
        "# Loop through each image and record performance\n",
        "for img_path in images:\n",
        "    i += 1\n",
        "\n",
        "    # Timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run pre loaded model prediction\n",
        "    results = model.predict(img_path)\n",
        "    result = results[0]  # Get the first result\n",
        "\n",
        "    # Total processing time for each image\n",
        "    end_time = time.time()\n",
        "    total_time = round(end_time - start_time, 2)\n",
        "\n",
        "    # Display the image\n",
        "    annotated_image = result.plot()\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.axis('off')  # Turn off the axes for better visualization\n",
        "    plt.show()\n",
        "\n",
        "    # Save the annotated image\n",
        "    annotated_image_pil = Image.fromarray(annotated_image)\n",
        "    annotated_image_pil.save(f\"image_{i}_annotated.jpg\")\n",
        "\n",
        "    # Object detection information\n",
        "    object_count = 0\n",
        "    total_confidences = []\n",
        "    detected_objects = []\n",
        "\n",
        "    # Loop through all detected objects in the image\n",
        "    for box in result.boxes:\n",
        "        object_count += 1\n",
        "        class_id = result.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "\n",
        "        # Store confidence\n",
        "        total_confidences.append(conf)\n",
        "        detected_objects.append({\n",
        "            \"class_id\": class_id,\n",
        "            \"coordinates\": cords,\n",
        "            \"confidence\": conf\n",
        "        })\n",
        "\n",
        "        # Print\n",
        "        print(f\"Image {i} - Object {object_count}\")\n",
        "        print(\"Object type:\", class_id)  # The ID of object type\n",
        "        print(\"Coordinates:\", cords)  # The coordinates of the box as an array [x1,y1,x2,y2]\n",
        "        print(\"Probability:\", conf)  # The confidence level of the model\n",
        "        print(\"\\nNext Object\")\n",
        "\n",
        "    # Store data\n",
        "    performance_data.append({\n",
        "        \"Image\": f\"image_{i}\",\n",
        "        \"Time Taken (s)\": total_time,\n",
        "        \"Objects Detected\": object_count,\n",
        "        \"Average Confidence\": round(sum(total_confidences) / len(total_confidences), 2) if total_confidences else 0,\n",
        "        \"All Detected Objects\": detected_objects  # Store all the object information for the image\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to present the results\n",
        "df = pd.DataFrame(performance_data)\n",
        "\n",
        "# Display the DataFrame in the console\n",
        "print(\"\\n--- Object Detection Performance Metrics ---\\n\")\n",
        "print(df)\n",
        "\n",
        "# Optional: Save the DataFrame to a CSV file\n",
        "csv_file_name = \"object_detection_metrics.csv\"\n",
        "df.to_csv(csv_file_name, index=False)\n",
        "print(f\"\\nPerformance data has been saved as '{csv_file_name}'.\")"
      ],
      "metadata": {
        "id": "wReoq1gQFJYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "3V6KPXeWFkTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.models.detection as detection\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "PPlcaZ2_FnzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "# Load the metadata for the pre-trained model\n",
        "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "COCO_CLASSES = weights.meta[\"categories\"]  # Fetch the class names dynamically\n",
        "\n",
        "print(COCO_CLASSES)"
      ],
      "metadata": {
        "id": "ZqaSVG5WFsM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision.models.detection as detection\n",
        "import matplotlib.patches as patches\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# COCO_CLASSES\n",
        "COCO_CLASSES = {i: f'class_{i}' for i in range(91)}\n",
        "\n",
        "# Load Faster R-CNN model\n",
        "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "image_paths = images\n",
        "\n",
        "# Data storage\n",
        "performance_data = []\n",
        "\n",
        "# Loop through each image\n",
        "for i, img_path in enumerate(image_paths, start=1):\n",
        "\n",
        "    print(f\"\\n--- Processing Image {i}: {img_path} ---\\n\")\n",
        "\n",
        "    # Load the image\n",
        "    image = Image.open(img_path)\n",
        "    image_tensor = to_tensor(image)  # Corrected to use the proper import for to_tensor\n",
        "\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform object detection\n",
        "    with torch.no_grad():\n",
        "        predictions = model([image_tensor])\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = round(end_time - start_time, 2)  # Total processing time for the image\n",
        "\n",
        "    # Extract predictions\n",
        "    predicted_boxes = predictions[0]['boxes']\n",
        "    predicted_scores = predictions[0]['scores']\n",
        "    predicted_labels = predictions[0]['labels']\n",
        "\n",
        "    filtered_boxes = []\n",
        "    filtered_scores = []\n",
        "    filtered_labels = []\n",
        "\n",
        "    confidence_threshold = 0.5  # Only keep predictions with confidence > 0.5\n",
        "\n",
        "    # Filter predictions\n",
        "    for box, score, label in zip(predicted_boxes, predicted_scores, predicted_labels):\n",
        "        if score >= confidence_threshold:\n",
        "            filtered_boxes.append(box.tolist())\n",
        "            filtered_scores.append(score.item())\n",
        "            filtered_labels.append(label.item())\n",
        "\n",
        "    # Annotate the image\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8), dpi=72)\n",
        "    ax.imshow(image)\n",
        "\n",
        "    detected_objects = []  # Store information\n",
        "    for box, score, label in zip(filtered_boxes, filtered_scores, filtered_labels):\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        class_name = COCO_CLASSES.get(label, f'class_{label}')\n",
        "\n",
        "        # Draw the bounding box\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min),\n",
        "            x_max - x_min,\n",
        "            y_max - y_min,\n",
        "            linewidth=2,\n",
        "            edgecolor='red',\n",
        "            facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add the class name\n",
        "        ax.text(\n",
        "            x_min,\n",
        "            y_min - 5,\n",
        "            f\"{class_name}: {score:.2f}\",\n",
        "            color='white',\n",
        "            fontsize=10,\n",
        "            bbox=dict(facecolor='red', alpha=0.5)\n",
        "        )\n",
        "\n",
        "        # Store object detection details\n",
        "        detected_objects.append({\n",
        "            \"class_name\": class_name,\n",
        "            \"bounding_box\": [round(x_min), round(y_min), round(x_max), round(y_max)],\n",
        "            \"confidence\": round(score, 2)\n",
        "        })\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the annotated image\n",
        "    annotated_image_path = f\"fasterrcnn_image_{i}_annotated.jpg\"\n",
        "    fig.savefig(annotated_image_path)\n",
        "    print(f\"Annotated image saved as: {annotated_image_path}\\n\")\n",
        "\n",
        "    # Close the plot\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Store performance data\n",
        "    object_count = len(detected_objects)\n",
        "    avg_confidence = round(sum([obj['confidence'] for obj in detected_objects]) / object_count, 2) if object_count > 0 else 0\n",
        "\n",
        "    performance_data.append({\n",
        "        \"Image\": f\"fasterrcnn_image_{i}\",\n",
        "        \"Time Taken (s)\": total_time,\n",
        "        \"Objects Detected\": object_count,\n",
        "        \"Average Confidence\": avg_confidence,\n",
        "        \"All Detected Objects\": detected_objects\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to present the results\n",
        "df = pd.DataFrame(performance_data)\n",
        "\n",
        "# Display the DataFrame in the console\n",
        "print(\"\\n--- Object Detection Performance Metrics ---\\n\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "LF1VNeZBGemp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part II - more analysis without using deep learning algorithms\n",
        "# Used grayscale to detect image edges, converted images to black and white (binary)\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "image = cv2.imread('image2.jpeg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "image = cv2.imread('image3.jpeg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "image = cv2.imread('image4.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "image = cv2.imread('image5.jpeg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "image = cv2.imread('image6.jpeg', cv2.IMREAD_GRAYSCALE)\n",
        "_, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Zix3NN99EJZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyzed two deep learning methods, YOLO and RCNN\n",
        "# RCNN was much more accurate with some objects coming in at 100% confidence\n",
        "# RCNN also took longer to load than YOLO, processes each region where YOLO processes the entire image at once\n",
        "# RCNN is better to use when the analyst is prioritizing accuracy over speed and YOLO is better for quick results\n",
        "\n",
        "\n",
        "# Part II used grayscale for non deep learning analysis\n",
        "# Creates binary images (black and white) for edge detection\n"
      ],
      "metadata": {
        "id": "-CU8U6eGF9I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a comparison table for the yolo and rcnn analysis of the images with real results from this colab with average time and accuracy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Method': ['YOLO', 'Faster R-CNN', 'Grayscale Edge Detection'],\n",
        "    'Average Time (s)': [0.5,  2.0, 0.1], # Replace with actual average times from your code\n",
        "    'Average Accuracy': [0.85, 0.95, 0.60], # Replace with actual average accuracies (or relevant metric)\n",
        "    'Accuracy Notes': ['Good speed, moderate accuracy', 'High accuracy but slower', 'Simple, fast, but less accurate'],\n",
        "    'Object Detection Notes': ['Processes whole image at once', 'Processes regions', 'Finds edges/contours; no object classification']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "Dmr5RzZ6M7MW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}